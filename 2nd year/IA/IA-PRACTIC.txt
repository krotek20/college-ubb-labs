80% din date sunt de antrenament, 20% sunt de test

y = mx + b

m - panta
b - punctul de intersectie cu Oy


Metoda celor mai mici patrate

m = sum_1..n((xi - xmediu)*(yi - ymediu)) / sum_1..n((xi - xmediu)^2)
n - numarul de date de antrenament
xi - Ox reference (temperatura)
yi - Oy reference (inghetate)
xmediu = x1+..+xn / n
ymediu = y1+..+yn / n

b = ymediu - m*xmediu


GD
learningRate = 0.01 ... 0.05
pas1 : valori random pt m si b (sau 0)
pas2 : m = m + deltam, b = b + deltab
error = guess - y
guess = mi*x + bi
cost = sum_1..n((guess - yi)^2)
I = error^2
di/dm (derivata in functie de m) = 2 * error * derror/dm
deltam = 2 * error * x
deltab = 2 * error * derror/db = 2 * error

practic:
m(t+1) = m(t) - learningRate * error(t) * x
b(t+1) = b(t) - learningRate * error(t)
error(t) = computed - realOutput
		 = b(t) = m1(t)*x1 + ... + md(t)*xd - y

Regresie logistica (clasificare)
exact ca GD, doar ca eroarea e calculata diferit
error(t) = Sigmoid(computed) - realOutput
		 = Sigmoid(b(t) = m1(t)*x1 + ... + md(t)*xd - y)
Sigmoid(x) = 1 / 1 + e^(-x) 

3 inp - 1 out
w1 = 0.5
w2 = 1
w3 = 0
out_calc(ex1) = w1 * inp1 + w2 * inp2 + w3 * inp3 = 0.9
out_expected(ex1) = 15
error = (15-0.9)^2 / 2
w1' = w1 - learningRate * error * inp1(ex1)
w2' = w2 - learningRate * error * inp2(ex1)
w3' = w3 - learningRate * error * inp3(ex1)

la mai multe layere
D (pe strat hidden)
inD = w11 * inp1 + w21 * inp2 + w31 * inp3 = outD
la final se calc eroarea
(outH - target)^2 / 2 = errorH
w_FH ' = w_FH - learningRate * errorH * outF
errorF = w_FH ' * errorH
w_DF ' = w_DF - learningRate * errorF * outD
pe acelasi strat cu F mai e si G
daca ne intoarcem in D trebuie calculata eroarea astfel
errorD = w_DF * errorF + w_DG * errorG (asincron, neactualizat)
errorD = w_DF ' * errorF + w_DG ' * errorG (sincron, actualizat, tuburile au dimensiunile diferite la intoarcere)